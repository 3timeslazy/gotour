Arrays
Arrays sind eine spezielle Datenstruktur in Go, die es uns ermöglicht, zusammenhängende Blöcke mit fester Speichergröße zuzuweisen.

* Arrays

- [[https://www.ardanlabs.com/training/individual-on-demand/ultimate-go-bundle/][Das Video ansehen]]
- Benötigen Sie finanzielle Unterstützung, nutzen Sie unser [[https://www.ardanlabs.com/scholarship/][Stipendienformular]]

Arrays sind eine spezielle Datenstruktur in Go, die es uns ermöglicht, zusammenhängende
Blöcke fester Größe zuzuweisen. Arrays haben in Go einige besondere Eigenschaften in Bezug auf
wie sie deklariert und als Typen betrachtet werden.

** Code-Überprüfung

- *Beispiel* *1:* Deklarieren, initialisieren und iterieren
- *Beispiel* *2:* Arrays unterschiedlichen Typs
- *Beispiel* *3:* Zusammenhängende Speicherzuweisungen
- *Beispiel* *4:* Bereichsmechanik

.play arrays/example1.go
.play arrays/example2.go
.play arrays/example3.go
.play arrays/example4.go

** Deklarieren und Initialisieren von Werten

Deklarieren Sie ein Array mit fünf Strings, die mit dem Wert Null initialisiert werden.

    var strings [5]string

Eine Zeichenkette ist eine unveränderliche Datenstruktur mit zwei Wörtern, die einen Zeiger auf ein
Backing-Array von Bytes und die Gesamtzahl der Bytes im Backing-Array darstellt. Da
dieses Array auf seinen Nullwertstatus gesetzt wird, wird jedes Element auf seinen Nullwert
Zustand gesetzt. Das bedeutet, dass bei jeder Zeichenkette das erste Wort auf Null und das zweite
Wort auf 0 gesetzt.

.image /tour/eng/static/img/a1.png

** String-Zuweisungen

Was passiert, wenn eine Zeichenkette einer anderen Zeichenkette zugewiesen wird?

    strings[0] = "Apfel"

Wenn eine Zeichenkette einer anderen Zeichenkette zugewiesen wird, wird der Zweiwortwert kopiert,
was dazu führt, dass zwei verschiedene Zeichenkettenwerte das gleiche Backing-Array teilen.

.image /tour/eng/static/img/a2.png

Die Kosten für das Kopieren einer Zeichenkette sind unabhängig von der Größe der Zeichenkette gleich, eine
Zwei-Wort-Kopie.

** Iteration über Sammlungen

Go bietet zwei verschiedene Semantiken für die Iteration über eine Sammlung. Ich kann iterieren
mit der Wert-Semantik oder der Zeiger-Semantik.

    // Wertesemantische Iteration
    for i, fruit := Bereich strings {
        println(i, fruit)
    }


    // Semantische Iteration von Zeigern
    for i := Bereich strings {
        println(i, strings[i])
    }

Bei der Verwendung der wertesemantischen Iteration geschehen zwei Dinge. Erstens wird die Sammlung, über die ich
über die ich iteriere, kopiert und Sie iterieren über diese Kopie. Im Falle eines Arrays kann die
kann die Kopie teuer sein, da das gesamte Array kopiert wird. Im Falle eines Slice,
gibt es keine wirklichen Kosten, da nur der interne Slice-Wert kopiert wird und nicht das
Hintergrund-Array kopiert wird. Zweitens erhalten Sie eine Kopie von jedem Element, über das iteriert wird.

Bei der semantischen Iteration mit Zeigern iterieren Sie über die ursprüngliche Sammlung und I
greifen direkt auf jedes mit der Sammlung verbundene Element zu.

** Wert Semantische Iteration

Geben Sie den folgenden Code und die folgende Ausgabe ein.

    strings := [5]string{"Apfel", "Orange", "Banane", "Traube", "Pflaume"}
    for i, fruit := range strings {
        println(i, fruit)
    }

Ausgabe:

    0 Apfel
    1 Orange
    2 Banane
    3 Weintraube
    4 Pflaume

Die Variable strings ist ein Array mit 5 Zeichenketten. Die Schleife iteriert über jede Zeichenkette
in der Sammlung und zeigt die Indexposition und den Wert der Zeichenkette an. Da dies
wertesemantische Iteration ist, iteriert der for-Bereich über seine eigene flache Kopie
des Arrays und bei jeder Iteration ist die Fruchtvariable eine Kopie jeder Zeichenkette
(die Zwei-Wort-Datenstruktur).

Beachten Sie, dass die Variable fruit mit Hilfe der Wertesemantik an die Druckfunktion übergeben wird.
Die Druckfunktion erhält ebenfalls eine eigene Kopie des Zeichenkettenwerts. Zu dem Zeitpunkt
die Zeichenkette an die Druckfunktion übergeben wird, gibt es 4 Kopien des Zeichenkettenwerts
(Array, oberflächliche Kopie, Fruchtvariable und die Kopie der Druckfunktion). Alle 4 Kopien
teilen sich dasselbe Backing-Array aus Bytes.

.image /tour/eng/static/img/a3.png

Die Erstellung von Kopien des Zeichenkettenwerts ist wichtig, da sie verhindert, dass der Zeichenkettenwert
nicht in den Heap entweicht. Dadurch wird eine unproduktive Zuweisung auf dem Heap vermieden.

** Semantische Iteration von Zeigern

Geben Sie den folgenden Code und die folgende Ausgabe ein.

    strings := [5]string{"Apfel", "Orange", "Banane", "Traube", "Pflaume"}
    for i := range strings {
        println(i, strings[i])
    }

Ausgabe:

    0 Apfel
    1 Orange
    2 Banane
    3 Weintraube
    4 Pflaume

Auch hier ist die Variable strings ein Array mit 5 Zeichenketten. Die Schleife iteriert über
jede Zeichenkette in der Sammlung und zeigt die Indexposition und den Wert der Zeichenkette an.
Da es sich um eine semantische Iteration mit Zeigern handelt, iteriert der for-Bereich direkt über das
Strings-Array und bei jeder Iteration wird für den Druckaufruf direkt auf den String-Wert für jede Indexposition
Position direkt für den Druckaufruf abgerufen.

** Verschiedene Arten von Arrays

Es ist interessant zu sehen, was der Compiler als Fehler meldet, wenn er
Arrays desselben Typs, die unterschiedlich lang sind.

    var fünf [5]int
    vier := [4]int{10, 20, 30, 40}

    fünf = vier

Compiler-Fehler:

    kann vier (Typ [4]int) nicht als Typ [5]int in einer Zuweisung verwenden

Hier deklarieren Sie ein Array mit 4 und 5 Ganzzahlen, die mit dem Wert Null initialisiert sind.
Dann versuchen Sie, sie einander zuzuweisen, und der Compiler sagt: "cannot use four
(Typ [4]int) als Typ [5]int in Zuweisung".

Es ist wichtig, dass Sie sich darüber im Klaren sind, was der Compiler sagt. Er sagt, dass ein
Array aus 4 Ganzzahlen und ein Array aus 5 Ganzzahlen Daten unterschiedlichen Typs darstellen.
Die Größe eines Arrays ist Teil der Typinformation. In Go muss die Größe eines Arrays
zur Kompilierzeit bekannt sein.

** Zusammenhängender Speicheraufbau

Sie wollen beweisen, dass ein Array ein zusammenhängendes Layout des Speichers bietet.

    five := [5]string{"Annie", "Betty", "Charley", "Doug", "Bill"}

    for i, v := range five {
        fmt.Printf("Wert[%s]\tAdresse[%p] IndexAddr[%p]\n",
            v, &v, &five[i])
    }

Ausgabe:

    Wert[Annie] Adresse[0xc000010250] IndexAddr[0xc000052180]
    Wert[Betty] Adresse[0xc000010250] IndexAddr[0xc000052190]
    Wert[Charley] Adresse[0xc000010250] IndexAddr[0xc0000521a0]
    Wert[Doug] Adresse[0xc000010250] IndexAddr[0xc0000521b0]
    Wert[Bill] Adresse[0xc000010250] IndexAddr[0xc0000521c0]

Hier deklarieren Sie ein Array mit 5 Strings, die mit Werten initialisiert werden. Dann verwenden Sie value
semantische Iteration, um Informationen über jede Zeichenkette anzuzeigen. Die Ausgabe zeigt
die Adresse der Variablen v und die Adresse jedes Elements im Array.
jedes Elements im Array.

Sie können sehen, dass das Array ein zusammenhängender Speicherblock ist und dass eine Zeichenkette eine Zwei
Wort- oder 16-Byte-Datenstruktur auf meiner 64-Bit-Architektur ist. Die Adresse für jedes Element
ist auf einem 16-Byte-Stride verteilt.

Die Tatsache, dass die Variable v bei jeder Iteration die gleiche Adresse hat, verstärkt das
Verständnis, dass v eine lokale Variable vom Typ String ist, die eine Kopie jedes
String-Wertes während der Iteration enthält.

** CPU-Caches

Es gibt viele mechanische Unterschiede zwischen den Prozessoren und ihrem Design. In
diesem Abschnitt werden Sie auf einer hohen Ebene über Prozessoren und die Semantik sprechen, die
die bei allen relativ gleich ist. Dieses semantische Verständnis wird Ihnen
ein gutes mentales Modell für die Funktionsweise des Prozessors und die Sympathie, die Sie ihm entgegenbringen können.

Jeder Kern innerhalb des Prozessors hat seinen eigenen lokalen Cache-Speicher (L1 und L2) und einen
gemeinsamen Cache-Speicher (L3), in dem Daten und Befehle gespeichert und abgerufen werden. Die Hardware
Threads in jedem Kern können auf ihre lokalen L1- und L2-Caches zugreifen. Daten aus L3 oder dem Hauptspeicher
Speicher müssen für den Zugriff in den L1- oder L2-Cache kopiert werden.

.image /tour/eng/static/img/a4.png

Die Latenzkosten für den Zugriff auf Daten, die sich in den verschiedenen Caches befinden, ändern sich von
geringsten zu den höchsten: L1 -> L2 -> L3 -> Hauptspeicher. Wie Scott Meyers sagte: "Wenn Leistung
zählt, dann ist die Gesamtmenge des Speichers die Gesamtmenge des Cache. Hauptspeicher
Der Hauptspeicher ist so langsam, dass er praktisch gar nicht vorhanden ist."

Bei der Leistung geht es heute darum, wie effizient Daten durch die Hardware fließen. Wenn alle
Daten, die die Hardware (zu einem bestimmten Zeitpunkt) benötigt, nur im Hauptspeicher vorhanden sind, werden meine
Programme langsamer laufen, als wenn die Daten bereits in den L1- oder L2-Caches vorhanden sind.

    3GHz(3 Taktzyklen/ns) * 4 Anweisungen pro Zyklus = 12 Anweisungen pro ns!

    1 ns ............. 1 ns .............. 12 Anweisungen (eine)
    1 µs ......... 1.000 ns .......... 12.000 Anweisungen (tausend)
    1 ms ..... 1.000.000 ns ...... 12.000.000 Befehle (Million)
    1 s .. 1.000.000.000 ns .. 12.000.000.000 Befehle (Milliarde)

    Von der Industrie definierte Latenzen
    L1-Cache-Referenz ......................... 0,5 ns ...................  6 ns
    L2-Cache-Referenz ........................... 7 ns ................... 84 ns
    Hauptspeicher-Referenz ...................... 100 ns ................. 1200 ins

Wie schreibt man Code, der garantiert, dass die Daten, die zur Ausführung eines Befehls benötigt werden
immer in den L1- oder L2-Caches vorhanden sind? Sie müssen einen Code schreiben, der mechanisch
mit dem Prefetcher des Prozessors zusammenarbeitet. Der Prefetcher versucht vorherzusagen, welche
vorhersagen, welche Daten benötigt werden, bevor die Befehle die Daten anfordern, so dass sie bereits entweder im
dem L1- oder L2-Cache vorhanden sind.

Es gibt verschiedene Granularitäten des Speicherzugriffs, je nachdem, wo der Zugriff erfolgt
geschieht. Mein Code kann ein Byte Speicher als kleinste Einheit des Speicherzugriffs lesen/schreiben
zugreifen. Aus der Sicht der Cache-Systeme beträgt die Granularität jedoch 64 Byte.
Dieser 64-Byte-Block des Speichers wird als Cache-Zeile bezeichnet.

Der Prefetcher funktioniert am besten, wenn die auszuführenden Anweisungen vorhersehbare
Zugriffsmuster auf den Speicher erzeugen. Eine Möglichkeit, ein vorhersehbares Zugriffsmuster auf den Speicher zu erzeugen
ist es, einen zusammenhängenden Speicherblock zu konstruieren und dann über diesen Speicher zu iterieren
einen linearen Durchlauf mit einem vorhersagbaren Schritt durchzuführen.

Das Array ist die wichtigste Datenstruktur für die Hardware, da es
vorhersehbare Zugriffsmuster unterstützt. Allerdings ist das Slice die wichtigste Datenstruktur
in Go. Slices in Go verwenden ein darunter liegendes Array.

Sobald Sie ein Array aufgebaut haben, ist jedes Element gleich weit vom nächsten oder
vorherigen Element. Wenn Sie über ein Array iterieren, beginnen Sie, Cache-Zeile für
verbundene Cache-Zeile in einem vorhersehbaren Schritt. Der Prefetcher wird dieses
vorhersehbaren Datenzugriffsmuster auf und beginnt, die Daten effizient in den Prozessor zu ziehen
Prozessor zu ziehen und so die Latenzkosten für den Datenzugriff zu reduzieren.

Stellen Sie sich vor, Sie haben eine große quadratische Speichermatrix und eine verknüpfte Liste von Knoten, die der
der Anzahl der Elemente in der Matrix entspricht. Wenn Sie einen Traversal über die verknüpfte
Liste durchführen und dann die Matrix in beide Richtungen (Spalte und Zeile) durchlaufen, wie wird
die Leistung der verschiedenen Traversals vergleichen?

    func RowTraverse() int {
        var ctr int
        for row := 0; row < rows; row++ {
            for col := 0; col < cols; col++ {
                if matrix[row][col] == 0xFF {
                    ctr++
                }
            }
        }
        return ctr
    }

Row Traverse hat die beste Leistung, weil es den Speicher Cache
Cache-Zeile für Cache-Zeile durchläuft, wodurch ein vorhersehbares Zugriffsmuster entsteht. Cache-Zeilen
können im Voraus abgerufen und in den L1- oder L2-Cache kopiert werden, bevor die Daten benötigt werden.

    func ColumnTraverse() int {
        var ctr int
        for col := 0; col < cols; col++ {
            for row := 0; row < rows; row++ {
                if matrix[row][col] == 0xFF {
                    ctr++
                }
            }
        }
        return ctr
    }

Column Traverse ist um eine Größenordnung schlechter, weil dieses Zugriffsmuster
bei jedem Speicherzugriff die OS-Seitengrenzen überquert. Dies führt zu keiner Vorhersagbarkeit
für das Prefetching von Cache-Zeilen und wird im Wesentlichen zu einem Speicher mit wahlfreiem Zugriff.

    func LinkedListTraverse() int {
        var ctr int
        d := Liste
        for d != nil {
            if d.v == 0xFF {
                ctr++
            }
            d = d.p
        }
        return ctr
    }

Die verknüpfte Liste ist doppelt so langsam wie die Zeilentraversierung, vor allem weil es Cache
Zeilenfehler, aber weniger TLB-Fehler (Translation Lookaside Buffer). Ein Großteil der Knoten
die in der Liste verbunden sind, befinden sich in denselben Betriebssystemseiten.

    BenchmarkLinkListTraverse-16 128 28738407 ns/op
    BenchmarkColumnTraverse-16 30 126878630 ns/op
    BenchmarkRowTraverse-16 310 11060883 ns/op

** Übersetzungs-Lookaside-Puffer (TLB)

Jedes laufende Programm erhält vom Betriebssystem eine vollständige Speicherabbildung des virtuellen Speichers, und das
laufende Programm denkt, dass es über den gesamten physischen Speicher des Rechners verfügt. Das ist aber nicht der Fall,
muss der physische Speicher jedoch mit allen laufenden Programmen geteilt werden. Das Betriebssystem
teilt den physischen Speicher, indem es den physischen Speicher in Seiten aufteilt und die Seiten
auf den virtuellen Speicher für ein bestimmtes laufendes Programm. Jedes Betriebssystem kann die Größe einer Seite selbst bestimmen,
aber 4k, 8k, 16k sind vernünftige und übliche Größen.

Der TLB ist ein kleiner Cache innerhalb des Prozessors, der dazu beiträgt, die Latenzzeit bei der
Übersetzung einer virtuellen Adresse in eine physikalische Adresse im Rahmen einer Betriebssystemseite
und dem Offset innerhalb der Seite. Ein Fehler im TLB-Cache kann große Latenzzeiten verursachen
weil die Hardware nun darauf warten muss, dass das Betriebssystem seine Seitentabelle durchsucht, um die richtige Seite für die virtuelle Adresse
die richtige Seite für die betreffende virtuelle Adresse zu finden. Läuft das Programm auf
einer virtuellen Maschine läuft (z. B. in der Cloud), muss die Auslagerungstabelle der virtuellen Maschine
zuerst gescannt werden.

Denken Sie daran, was gesagt wurde:

Die verknüpfte Liste ist doppelt so langsam wie der Zeilentraversal, hauptsächlich weil es Cache
Zeilenfehlern, aber weniger TLB-Fehlern kommt (siehe unten). Ein Großteil der in der Liste verbundenen Knoten
der Liste verbunden sind, befinden sich innerhalb derselben Betriebssystemseiten.

Die LinkedList ist aufgrund des TLB-Zugriffs um Größenordnungen schneller als das Spalten-Traversal.
TLB-Zugriffs. Auch wenn es beim LinkedList-Traversal zu Cache Line Misses kommt,
da ein Großteil des Speichers für eine Gruppe von Knoten in derselben Seite landet,
wirken sich TLB-Latenzen nicht auf die Leistung aus. Dies ist der Grund, warum bei Programmen, die eine
viel Speicher benötigen, wie DNA-basierte Anwendungen, sollten Sie eine Linux-Distribution verwenden
von Linux verwenden, die mit Seitengrößen in der Größenordnung von einem oder zwei Megabyte Speicher konfiguriert ist.

Dennoch ist ein datenorientierter Entwurf wichtig. Das Schreiben eines effizienten Algorithmus muss
berücksichtigen, wie auf die Daten zugegriffen wird. Denken Sie daran, dass es bei der Leistung heute darum geht
wie effizient man Daten in den Prozessor bringen kann.

- [[https://youtu.be/WDIkqP4JbkE?t=1129][CPU-Caches und warum Sie das interessiert (18:50-20:30)]] - Scott Meyers
- [[https://youtu.be/WDIkqP4JbkE?t=2676][CPU-Caches und warum Sie das interessiert (44:36-45:40)]] - Scott Meyers
- [[https://youtu.be/jEG4Qyo_4Bc?t=266][Leistung durch Cache-Freundlichkeit (4:25-5:48)]] - Damian Gryski

** Hinweise zum CPU-Cache

.html arrays/array_list.html

** Extra Diagramme

*Industrie* *Defined* *Latenzen*

    L1-Cache-Referenz ......................... 0,5 ns ...................  6 ins
    Zweigfehlvorhersage ............................ 5 ns ................... 60 ns
    L2-Cache-Referenz ........................... 7 ns ................... 84 ns
    Mutex sperren/entsperren ........................... 25 ns .................. 300 ns
    Hauptspeicher-Referenz ...................... 100 ns ................. 1200 ns
    Komprimierung von 1K Bytes mit Zippy ............. 3.000 ns (3 µs) ........... 36k ins
    Senden von 2K Bytes über ein 1-Gbit/s-Netzwerk ....... 20.000 ns (20 µs) ........ 240k ins
    Zufälliges Lesen von SSDs ........................ 150.000 ns (150 µs) ........ 1.8M ins
    Lesen von 1 MB sequentiell aus dem Speicher ..... 250.000 ns (250 µs) .......... 3M ins
    Hin- und Rückfahrt innerhalb desselben Rechenzentrums ...... 500.000 ns (0,5 ms) .......... 6M ins
    Lesen von 1 MB sequentiell von SSD- ..... 1.000.000 ns (1 ms) ........... 12M ins
    Festplattensuche ........................... 10.000.000 ns (10 ms) ......... 120M ins
    1 MB sequentiell von der Festplatte lesen .... 20.000.000 ns (20 ms) ......... 240M ins
    Senden eines Pakets CA->Niederlande->CA .... 150.000.000 ns (150 ms) ........ 1,8B ins

*Cache* *Latenzzeiten* *Bild*

.image /tour/eng/static/img/cache_latencies_graph.png

** Extra Lesen

*CPU* *Caches* */* *Speicher*

- [[https://www.youtube.com/watch?v=WDIkqP4JbkE][CPU-Caches und warum Sie das interessiert - Video]] - Scott Meyers
- [[https://www.youtube.com/watch?v=OFgxAFdxYAQ][Ein Crashkurs in moderner Hardware - Video]] - Cliff Click
- [[http://frankdenneman.nl/2016/07/06/introduction-2016-numa-deep-dive-series/][NUMA Deep Dive Reihe]] - Frank Denneman
- [[http://www.aristeia.com/TalkNotes/codedive-CPUCachesHandouts.pdf][CPU-Caches und warum Sie das interessiert - Deck]] - Scott Meyers
- [[https://www.youtube.com/watch?v=MC1EKLQ2Wmg][Mythbusting moderner Hardware zur Gewinnung von 'mechanischer Sympathie']] - Martin Thompson
- [[http://www.akkadia.org/drepper/cpumemory.pdf][Was jeder Programmierer über Speicher wissen sollte]] - Ulrich Drepper
- [[http://www.extremetech.com/extreme/188776-how-l1-and-l2-cpu-caches-work-and-why-theyre-an-essential-part-of-modern-chips][Wie CPU-Caches funktionieren und warum]] - Joel Hruska
- [[http://www.lighterra.com/papers/modernmicroprocessors][Moderne Mikroprozessoren - ein 90-Minuten-Leitfaden]] - Jason Robert Carey Patterson
- [[http://lwn.net/Articles/252125][Speicher Teil 2: CPU-Caches]] - Ulrich Drepper
- [[http://www.gotw.ca/publications/concurrency-ddj.htm][Das kostenlose Mittagessen ist vorbei]] - Herb Sutter
- [[https://m.youtube.com/watch?feature=youtu.be&v=QBu2Ae8-8LM][Computer für Rechenzentren: Moderne Herausforderungen beim CPU-Design]] - Dick Sites
- [[https://en.wikipedia.org/wiki/Wirth%27s_law][Wirths Gesetz]] - Wikipedia
- [[http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206][Falsches Teilen eliminieren]] - Herb Sutter
- [[http://www.ilikebigbits.com/2014_04_21_myth_of_ram_1.html][Der Mythos vom Widder]] - Emil Ernerfeldt
- [[https://www.infoq.com/presentations/hardware-transactional-memory][Verstehen von Transaktions-Hardware-Speicher]] - Gil Gene
- [[https://youtu.be/jEG4Qyo_4Bc?t=266][Leistung durch Cache-Freundlichkeit (4:25-5:48)]] - Damian Gryski
- [[https://www.youtube.com/watch?v=2EWejmkKlxs][Nirgendwo schneller hinkommen]] - Chandler Carruth

*Datenorientiert* *Design*

- [[https://www.youtube.com/watch?v=rX0ItVEVjHc][Datenorientiertes Design und C++]] - Mike Acton
- [[https://www.youtube.com/watch?v=fHNmRkzxHWs][Effizienz mit Algorithmen, Leistung mit Datenstrukturen]] - Chandler Carruth
- [[https://www.youtube.com/watch?v=LrVi9LHP8Bk][Die Zähmung der Leistungsbestie]] - Klaus Iglberger
- [[http://harmful.cat-v.org/software/OO_programming/_pdf/Pitfalls_of_Object_Oriented_Programming_GCAP_09.pdf][Fallstricke der OOP]] - Tony Albrecht
- [[https://www.youtube.com/watch?v=YQs6IC-vgmo][Warum Sie Linked Lists vermeiden sollten]] - Bjarne Stroustrup
- [[http://gamesfromwithin.com/data-oriented-design][Datenorientiertes Design (oder warum Sie sich mit OOP selbst ins Bein schießen)]] - Noel
- [[https://www.quora.com/Was-object-oriented-programming-a-failure][War die objektorientierte Programmierung ein Fehlschlag?]] - Quora

** Anmerkungen

- Wenn man die Daten nicht versteht, versteht man das Problem nicht.
- Wenn man die Kosten für die Lösung des Problems nicht versteht, kann man nicht über das Problem nachdenken.
- Wenn man die Hardware nicht versteht, kann man auch nicht über die Kosten für die Lösung des Problems nachdenken.
- Arrays sind Datenstrukturen mit fester Länge, die sich nicht ändern können.
- Arrays unterschiedlicher Größe werden als unterschiedliche Typen betrachtet.
- Speicher wird als zusammenhängender Block zugewiesen.
- Go gibt Ihnen die Kontrolle über die räumliche Lokalität.

* Übungen

Verwenden Sie die Vorlage als Ausgangspunkt, um die Aufgaben zu lösen. Eine mögliche Lösung
ist vorhanden.

** Übung 1

Deklarieren Sie ein Array mit 5 Strings, wobei jedes Element mit dem Wert Null initialisiert wird. Deklarieren Sie
ein zweites Array mit 5 Strings und initialisiere dieses Array mit literalen Stringwerten. Weisen Sie
das zweite Array dem ersten zu und zeige die Ergebnisse des ersten Arrays an. Zeigen Sie den
String-Wert und die Adresse jedes Elements an.

.play arrays/exercise1.go
.play arrays/answer1.go
